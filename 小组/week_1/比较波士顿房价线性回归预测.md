# 调用sklearn库

## 导入必要的库

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler

## 导入波士顿房价的数据集

data = pd.read_csv('house_data.csv')

##数据预处理

X = data.drop('MEDV', axis=1)  # 特征矩阵
y = data['MEDV']  # 目标变量

## 标准化特征

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

## 创建线性回归模型

lin_reg = LinearRegression()

## 拟合模型

lin_reg.fit(X_scaled, y)

## 获取回归系数和截距

theta_best_sklearn = lin_reg.coef_
intercept_best_sklearn = lin_reg.intercept_

## 打印模型参数

print(f"最小二乘法模型的回归系数：{theta_best_sklearn}")
print(f"最小二乘法模型的截距：{intercept_best_sklearn}")

## 预测值

predictions_sklearn = lin_reg.predict(X_scaled)

## 计算代价函数值 (均方误差)

cost_sklearn = np.mean((predictions_sklearn - y) ** 2)
print(f"均方误差 (MSE): {cost_sklearn}")

## 绘制预测值与真实值的对比

plt.figure(figsize=(10, 6))
plt.scatter(y, predictions_sklearn, color='green', label='预测值')
plt.plot([min(y), max(y)], [min(y), max(y)], color='red', label='实际值')
plt.xlabel('实际房价')
plt.ylabel('预测房价')
plt.title('最小二乘法回归：预测值与实际值对比')
plt.legend()
plt.show()



## 创建SGDRegressor模型（梯度下降法）

sgd_regressor = SGDRegressor(max_iter=1000, tol=1e-3, learning_rate='constant', eta0=0.01)

## 拟合模型

sgd_regressor.fit(X_scaled, y)

## 获取回归系数和截距

theta_best_sgd = sgd_regressor.coef_
intercept_best_sgd = sgd_regressor.intercept_

## 打印模型参数

print(f"SGDRegressor回归系数：{theta_best_sgd}")
print(f"SGDRegressor截距：{intercept_best_sgd}")

## 预测值

predictions_sgd = sgd_regressor.predict(X_scaled)

## 计算均方误差 (MSE)

cost_sgd = np.mean((predictions_sgd - y) ** 2)
print(f"均方误差 (MSE): {cost_sgd}")

## 绘制预测值与真实值的对比

plt.figure(figsize=(10, 6))
plt.scatter(y, predictions_sgd, color='green', label='预测值')
plt.plot([min(y), max(y)], [min(y), max(y)], color='red', label='实际值')
plt.xlabel('实际房价')
plt.ylabel('预测房价')
plt.title('SGDRegressor（梯度下降法）回归：预测值与实际值对比')
plt.legend()
plt.show()



# 手推

## 导入必要的库

import numpy as np import warnings warnings.filterwarnings('ignore')

#避免一些烦人的提示和警

#导入matplotlib库-绘图库

import matplotlib.pyplot as plt %matplotlib inline import pandas as pd

## 导入波士顿房价的数据集

data=pd.read_csv('house_data.csv')

## 数据预处理

X=data.drop('MEDV',axis=1) y=data['MEDV']

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

#标准化特征

X_scaled = scaler.fit_transform(X)

#添加截矩阵项，即特征矩阵X中加入一列全是1的列

X_b=np.c_[np.ones(X_scaled.shape[0]),X_scaled]

#X_b 为加了截距项的特征矩阵

#使用最小二乘法计算线性回归的最佳参数

#计算转置矩阵 X_b^T

X_b_T = X_b.T

#计算 X_b^T * X_b

X_b_T_X_b = X_b_T.dot(X_b)

#计算 (X_b^T * X_b) 的逆

X_b_T_X_b_inv = np.linalg.inv(X_b_T_X_b)

#计算 X_b^T * y，-dot() 内积，对应元素相乘求和

X_b_T_y = X_b_T.dot(y)

#最后计算参数 theta

theta_best = X_b_T_X_b_inv.dot(X_b_T_y)

#打印最小二乘法得到的模型参数

print(theta_best)

## 计算最小二乘法中的代价函数值

#样本数量

m=len(y)

#预测值  h(x)=X_b*theta

predictions=X_b.dot(theta_best)

#计算代价函数数值

cost = (1 / (2 * m)) * np.sum((predictions - y) ** 2) print(cost)

#多次用到代价函数，将其封装为函数

def cost_function(theta_best, X_b, y): # 样本数量 m=len(y) # 预测值  h(x)=X_b*theta predictions=X_b.dot(theta_best) # 计算代价函数数值 cost = (1 / (2 * m)) * np.sum((predictions - y) ** 2) # print(cost) return cost

#cost_function(theta_best, X, y)    测试代码，看函数能否运行

## 进行梯度下降算法优化

#定义学习率

learn_rate=0.01

#定义迭代次数

iterations=1000

#随机初始化权重

theta_initial = np.random.randn(X_b.shape[1])

def gradient_descent(X_b, y, theta_initial, learn_rate, iterations):

```
# 进行梯度下降法优化模型参数

m = len(y)  # 样本数量

theta_best=theta_initial.copy()

# 定义一个辅助列表 
# 存储每次迭代的代价函数值
cost_Front= []  

for _ in range(iterations):
    predictions = X_b.dot(theta_best)  # 预测值
    errors = predictions - y  # 预测误差
    theta_best =theta_best-(1 / m) * learn_rate * (X_b.T.dot(errors))  # 更新参数
    cost = cost_function(theta_best,X_b, y)   # 计算新的代价函数值
     # 存储代价函数值
    cost_Front.append(cost)  

return theta_best, cost_Front
```

#使用梯度下降法进行参数优化

theta_gradient, cost_Front = gradient_descent(X_b, y, theta_initial, learn_rate, iterations)

#打印梯度下降法得到的模型参数

print(f"梯度下降算法得到的权重：, {theta_gradient}")

#使用梯度下降算法得到的参数进行预测

predictions_gradient = X_b.dot(theta_gradient)

#打印结果

print(f"预测值：{predictions_gradient}")

#绘制梯度下降算法的代价函数变化

plt.plot(range(iterations), cost_Front,label='梯度下降算法')

plt.xlabel('迭代次数') plt.ylabel('代价函数') plt.title('梯度下降算法的代价函数变化') plt.legend() plt.show()

#绘图直观比较预测值与真实值的偏差

plt.figure(figsize=(10, 6))

plt.rcParams['font.sans-serif'] = ['SimHei']  # 用来正常显示中文标签 plt.rcParams['axes.unicode_minus'] = False    # 用来显示正常负号

plt.scatter(y, predictions, color='green', label='预测值') plt.plot([min(y), max(y)], [min(y), max(y)], color='red', label='实际值') plt.xlabel('实际房价') plt.ylabel('预测房价') plt.title('预测值与实际值对比')

#添加图例标签

plt.legend()
 plt.show() plt.figure(figsize=(10, 6))

plt.rcParams['font.sans-serif'] = ['SimHei']  # 用来正常显示中文标签 plt.rcParams['axes.unicode_minus'] = False    # 用来显示正常负号

plt.scatter(y, predictions_gradient, color='green', label='预测值') plt.plot([min(y), max(y)], [min(y), max(y)], color='red', label='实际值') plt.xlabel('实际房价') plt.ylabel('预测房价') plt.title('预测值与实际值对比')

#添加图例标签

plt.legend()
 plt.show()



# ==两者比较==

在对比sklearn的线性回归实现与手推实现的过程中，有一些关键区别和优化。

### 1. **数据预处理**

- **sklearn实现**：使用了`StandardScaler`进行特征标准化处理，这会将每个特征缩放到均值为0，标准差为1的范围。这样做有助于提高模型在不同特征尺度上的表现。
- **手推实现**：也使用了`StandardScaler`进行标准化，确保两个实现中都消除了特征的尺度差异。

### 2. **回归模型构建**

- **sklearn实现**：使用了`LinearRegression()`，它内部默认使用最小二乘法来求解回归系数。`fit()`函数通过封装的优化方法，直接求得最优的回归系数和截距。
- **手推实现**：手动实现了最小二乘法，使用了正规方程求解回归系数。通过矩阵运算，计算出了参数 `theta_best`，即 `(X^T * X)^(-1) * X^T * y`，这是一种解析解。

### 3. **优化方法**

- **sklearn实现**：sklearn的`LinearRegression`类提供了快速的最小二乘法计算，避免了手动矩阵求逆等繁琐步骤，背后通过高效的算法实现了参数估计。它还包括了对奇异矩阵的处理，因此更为稳健。
- **手推实现**：手动实现了正规方程的求解，虽然理论上是准确的，但在处理大规模数据时会遇到计算上的效率瓶颈，尤其是矩阵求逆操作。对于大数据集，可能需要额外考虑数值稳定性和计算复杂度。



### 4. **代码逻辑上的区别**

#### **sklearn实现：**

- **封装与简化**：`sklearn`的`LinearRegression`类封装了整个回归过程，使得用户可以通过非常简洁的代码调用API来实现回归分析。例如，用户只需要调用`fit(X, y)`方法来拟合模型，库内部自动处理所有数学计算、数据预处理、回归系数求解和优化过程。
- **标准化**：在`sklearn`中，特征标准化（如使用`StandardScaler`）是手动执行的，而不是`LinearRegression`类的内部一部分。这意味着特征标准化必须由用户明确执行。如果不进行标准化，可能会导致回归模型不稳定，特别是当特征尺度差异较大时。

#### **手推实现：**

- **手动实现每个步骤**：手推实现需要手动处理所有的计算，模型的训练过程包括数据的预处理、正规方程的矩阵运算、代价函数的计算、以及梯度下降的实现。每一个步骤都需要开发者显式地写出代码来完成。
- **正规方程与梯度下降**：手推代码实现了两个重要的优化方法：**正规方程**（解析解）和**梯度下降**（迭代优化）。你首先通过正规方程直接求得回归系数`theta_best`，然后通过梯度下降优化算法进一步优化模型。

### 5. **数学实现的区别**

#### **sklearn实现：**

- **最小二乘法**：`sklearn`中的`LinearRegression`类使用了最小二乘法来计算回归系数，这相当于通过矩阵运算直接求解
	$$
	\theta = (X^T X)^{-1} X^T y
	$$
	这种方法的数学核心是**正规方程**。

- **封装优化**：`sklearn`在实现上进行了很多优化，确保了数值稳定性和高效性，避免了矩阵求逆时的潜在问题。对于大规模数据，`sklearn`的优化方法会比手动实现更加高效（例如，采用梯度下降而非直接求逆）。

#### **手推实现：**

- **正规方程**：手推实现通过正规方程的方式求解回归系数，具体计算步骤是：
	$$
	\theta = (X^T X)^{-1} X^T y
	$$
	

	这里，`X^T * X` 表示特征矩阵的转置与特征矩阵的乘积，接着对这个矩阵求逆，再与特征矩阵的转置 `X^T` 和目标变量 `y` 做乘积，得到最终的回归系数。这是一个闭式解（解析解），不依赖于迭代优化过程。

- **梯度下降法**：手推实现还手动实现了梯度下降算法来进一步优化模型参数。梯度下降的更新规则是：
	$$
	\theta = \theta - \alpha \nabla J(\theta)
	$$

	#### **手推实现：**

	

	- **正规方程的限制**：手推实现使用了**矩阵求逆**，而对于大规模数据集，矩阵求逆可能导致计算效率低下和数值不稳定。这是因为矩阵的求逆对于非常大的矩阵可能会出现误差，尤其当矩阵条件数很大时。

	- **梯度下降的优势**：手推实现中的梯度下降方法是一种更灵活的优化方法，可以逐步接近最优解，而且适用于大规模数据集。虽然梯度下降的收敛速度和效果依赖于学习率和迭代次数，但它相比正规方程可以更好地适应大数据问题。